---
title: "Tekstanalyse ved bruk av tidytext"
author: "Theodor Bredal-Thorsen"
format:
  revealjs:
    code-overflow: wrap
    self-contained: true 
    embed-resources: true
    theme: serif
    width: 1280
    height: 720
    show-notes: false 
    slide-number: true
    transition: fade
    toc-depth: 2
    code-annotations: select
execute:
  echo: true
  warning: false
  message: false
editor: visual
---
# Oppgave 1 - Lese inn og strukturere tekst fra PDF

# Pakker
```{r}
library(tidytext)
library(quanteda)
library(purrr)
library(dplyr)
library(stringr)
library(pdftools)
```


# Fasit
```{r}
# Lese inn 3-4 dokumenter fra samme kilde i forskjellige tidsrom, med varierende innhold
filer <- list.files("C:/Dokumenter/repo/modul14/Oppgavefiler")


data <- tibble(
        dok_navn = filer,
        year = filer |> str_extract("[0-9]+"),
        tekst = map_chr(str_c("C:/Dokumenter/repo/modul14/Oppgavefiler/", filer),
          ~ pdf_text(.x) |>
            paste(collapse = "\n") |> 
            str_replace_all("\\s+", " ") |> 
            str_squish()
            ))

```


# Oppgave 2 - Dele teksten inn i analyseenheter


# Fasit
```{r}
# Teste ut å dele i ord, bigram og setninger

ord <- data |> unnest_tokens(word, tekst, token = "words")

bigram <- data |> 
  mutate(tekst = str_remove_all(tekst, "[0-9]+")) |> 
  unnest_tokens(to_ord, tekst, token = "ngrams", n = 2)

setninger <- data |> 
  unnest_tokens(setninger, tekst, token = "sentences")
  

```


# Oppgave 3 - Preprosessering av tekst

```{r}
# Redusere kompleksitet i tekst, følge standardoppskrift
ord <- ord |> 
  filter(!str_detect(word, "[0-9]+"),
         !str_detect(word, "[.!?,]")) |> 
    filter(str_length(word) > 1) |> 
    filter(!word %in% stopwords(language = "no")) #|> 
  #mutate(word = char_wordstem(word, language = "no")) #"Stemming", rotform av ordet


```


# Oppgave 4 - Ordboksanalyse
```{r}
# Finne ord som relaterer til en sak, sjekke opp mot korpus (Kanskje Covid?)

ord_telling <- ord |> 
  group_by(year) |> 
  count(word)

plot_data <- ord_telling |> 
  filter(word %in% c("kritikk", "kritikkverdig", "mangler", "manglende", "kritisk")) |> 
  group_by(year) |> 
  summarise(n = sum(n))

library(ggplot2)

plot <- ggplot(data = plot_data, aes(x = year, y = n)) +
  geom_col(fill = "firebrick", alpha =0.5) +
  theme_minimal()
```
# 
```{r}
plot
```
```{r}
str_length(data$tekst[1:3])

```

# Gir oftest mening med mange dokumenter!

```{r}

set.seed(501)

ord_boot <- ord$word

bootstrapped <- list()

for (i in c(1:25)) {
  
  boot_sample <- sample(
  ord_boot,
  replace = TRUE,
  size = 10000)
  
      bootstrapped[[i]] <- boot_sample
}

bootstrapped_data <- tibble(
  dok_navn = paste0("boot_", 1:25),
  year = paste0("year_", 1:25),
  tekst = map_chr(bootstrapped, ~ paste(.x, collapse = " "))
)

ord_boot <- bootstrapped_data |> 
  unnest_tokens(word, tekst)

ord_telling_boot <- ord_boot |> 
  group_by(year) |> 
  count(word)

plot_data_boot <- ord_telling_boot |> 
  filter(word %in% c("kritikk", "kritikkverdig", "mangler", "manglende", "kritisk")) |> 
  group_by(year) |> 
  summarise(n = sum(n))

plot_boot <- ggplot(data = plot_data_boot, aes(x = year, y = n)) +
  geom_col(fill = "firebrick", alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal()

plot_boot

```


